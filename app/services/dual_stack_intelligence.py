"""
Sistema Dual-Stack: Fine-tune + RAG Dirigido
Camada 1: Fine-tune pr√≥prio com Chain-of-Thought enxuto
Lat√™ncia total < 900ms para superar concorr√™ncia
"""
import asyncio
import json
import logging
import os
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
import hashlib
from dataclasses import dataclass
import re

from app.services.rag_pipeline import rag
from app.services.embedding_cache import embedding_cache

logger = logging.getLogger(__name__)

@dataclass
class PropertyHypothesis:
    """Hip√≥tese gerada pelo fine-tuned model"""
    neighborhood: Optional[str] = None
    min_price: Optional[float] = None
    max_price: Optional[float] = None
    bedrooms: Optional[int] = None
    property_type: Optional[str] = None  # casa, apartamento, comercial
    transaction_type: Optional[str] = None  # venda, locacao
    urgency_score: int = 1  # 1-5, onde 5 = <HOT>
    intent_confidence: float = 0.0
    extracted_keywords: List[str] = None

class DualStackIntelligence:
    """Sistema Dual-Stack para m√°xima performance"""
    
    def __init__(self):
        self.session_cache = {}  # Cache de sess√£o por phone_hash
        self.cache_ttl_hours = 24
        
        # Padr√µes para urg√™ncia
        self.urgency_patterns = [
            r'(preciso|tenho que) (sair|mudar|deixar).{0,20}(sexta|s√°bado|domingo|semana|m√™s)',
            r'(despej\w+|despejo|saindo de casa|sem lugar)',
            r'(urgente|emerg√™ncia|r√°pido|logo|j√°)',
            r'(at√©|antes d[eo]) (sexta|fim de semana|\d{1,2}\/\d{1,2})',
            r'(casamento|separa√ß√£o|trabalho novo|transfer√™ncia) (pr√≥xim\w+|em \w+ dias)'
        ]
        
        # Cache de hip√≥teses para evitar reprocessamento
        self.hypothesis_cache = {}
    
    async def process_dual_stack_query(self, 
                                     user_message: str, 
                                     user_phone: str,
                                     conversation_history: List[Dict] = None) -> Dict[str, Any]:
        """
        Chain-of-Thought enxuto:
        1. Fine-tuned model gera hip√≥tese
        2. Hip√≥tese vira query vetorial com filtros
        3. Recupera top-3 + 2 compar√°veis  
        4. Fine-tuned model reescreve resposta
        """
        start_time = datetime.utcnow()
        
        try:
            # 1. GERAR HIP√ìTESE (Camada 1: Fine-tune)
            hypothesis = await self._generate_hypothesis(user_message, conversation_history)
            
            # 2. CONSULTAR CACHE DE SESS√ÉO primeiro
            phone_hash = self._get_phone_hash(user_phone)
            cached_properties = self._get_session_cache(phone_hash, hypothesis)
            
            if cached_properties:
                logger.info(f"Cache HIT para {phone_hash}: {len(cached_properties)} propriedades")
                properties = cached_properties
            else:
                # 3. QUERY VETORIAL DIRIGIDA (Camada 2: RAG leve)
                properties = await self._directed_vector_search(hypothesis, user_message)
                
                # 4. SALVAR NO CACHE DE SESS√ÉO
                self._update_session_cache(phone_hash, properties, hypothesis)
            
            # 5. REESCREVER RESPOSTA COM FINE-TUNE
            final_response = await self._generate_top_seller_response(
                user_message, hypothesis, properties, conversation_history
            )
            
            # Calcular lat√™ncia
            latency_ms = (datetime.utcnow() - start_time).total_seconds() * 1000
            
            return {
                "response": final_response,
                "properties": properties[:3],  # Top 3 para UI
                "comparable_properties": properties[3:5] if len(properties) > 3 else [],
                "hypothesis": hypothesis,
                "urgency_detected": hypothesis.urgency_score >= 4,
                "latency_ms": latency_ms,
                "cache_hit": bool(cached_properties)
            }
            
        except Exception as e:
            logger.error(f"Erro no dual-stack: {e}")
            
            # Fallback para sistema original
            return await self._fallback_response(user_message, user_phone)
    
    async def _generate_hypothesis(self, 
                                  user_message: str, 
                                  history: List[Dict] = None) -> PropertyHypothesis:
        """Gera hip√≥tese estruturada via fine-tuned model"""
        
        # Cache de hip√≥tese
        msg_hash = hashlib.md5(user_message.encode()).hexdigest()[:8]
        if msg_hash in self.hypothesis_cache:
            return self.hypothesis_cache[msg_hash]
        
        # Prompt otimizado para Chain-of-Thought
        system_prompt = """
        Voc√™ √© Sofia, IA imobili√°ria top-vendedora. Analise a mensagem e extraia HIP√ìTESE estruturada.
        
        RETORNE JSON EXATO:
        {
          "neighborhood": "bairro ou null",
          "min_price": numero_ou_null,
          "max_price": numero_ou_null, 
          "bedrooms": numero_ou_null,
          "property_type": "apartamento|casa|comercial|null",
          "transaction_type": "venda|locacao|null",
          "urgency_score": 1-5,
          "intent_confidence": 0.0-1.0,
          "extracted_keywords": ["palavra1", "palavra2"]
        }
        
        URGENCY_SCORE:
        5 = URGENTE (preciso sair sexta, despejo, emerg√™ncia)
        4 = ALTA (casamento pr√≥ximo, trabalho novo)
        3 = M√âDIA (procurando ativamente)
        2 = BAIXA (s√≥ olhando)
        1 = M√çNIMA (curiosidade)
        """
        
        # Contexto hist√≥rico resumido
        context = ""
        if history:
            recent_msgs = history[-3:]  # √öltimas 3 mensagens
            context = "\\nHIST√ìRICO:\\n" + "\\n".join([
                f"{m.get('role', 'user')}: {m.get('content', '')[:100]}"
                for m in recent_msgs
            ])
        
        full_prompt = f"{system_prompt}\\n\\nMENSAGEM: \"{user_message}\"{context}\\n\\nJSON:"
        
        try:
            # Usar modelo fine-tuned
            model = os.getenv("OPENAI_FINETUNED_MODEL", "ft:gpt-4.1-mini-2025-04-14:personal:alloha-sofia-v1:CMFHyUpi")
            response = await asyncio.to_thread(rag.call_gpt, full_prompt, model)
            
            # Extrair JSON
            json_match = re.search(r'\\{.*\\}', response, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group())
                
                hypothesis = PropertyHypothesis(
                    neighborhood=data.get("neighborhood"),
                    min_price=data.get("min_price"),
                    max_price=data.get("max_price"),
                    bedrooms=data.get("bedrooms"),
                    property_type=data.get("property_type"),
                    transaction_type=data.get("transaction_type"),
                    urgency_score=data.get("urgency_score", 1),
                    intent_confidence=data.get("intent_confidence", 0.0),
                    extracted_keywords=data.get("extracted_keywords", [])
                )
                
                # Cache da hip√≥tese
                self.hypothesis_cache[msg_hash] = hypothesis
                
                logger.info(f"Hip√≥tese gerada: {hypothesis.neighborhood}, {hypothesis.property_type}, urg√™ncia={hypothesis.urgency_score}")
                return hypothesis
            
        except Exception as e:
            logger.debug(f"Erro ao gerar hip√≥tese: {e}")
        
        # Fallback: hip√≥tese b√°sica com regex
        return self._generate_fallback_hypothesis(user_message)
    
    def _generate_fallback_hypothesis(self, message: str) -> PropertyHypothesis:
        """Hip√≥tese de fallback usando regex"""
        
        msg_lower = message.lower()
        
        # Detectar urg√™ncia
        urgency_score = 1
        for pattern in self.urgency_patterns:
            if re.search(pattern, msg_lower, re.IGNORECASE):
                urgency_score = max(urgency_score, 4)
                break
        
        # Extrair bairros conhecidos
        neighborhoods = ["√°gua verde", "bigorrilho", "batel", "centro", "cabral", "jardins"]
        neighborhood = None
        for n in neighborhoods:
            if n in msg_lower:
                neighborhood = n.title()
                break
        
        # Extrair quartos
        bedrooms_match = re.search(r'(\\d+)\\s*(quarto|dormit√≥rio)', msg_lower)
        bedrooms = int(bedrooms_match.group(1)) if bedrooms_match else None
        
        # Tipo de im√≥vel
        property_type = None
        if any(t in msg_lower for t in ["apartamento", "apto", "ap"]):
            property_type = "apartamento"
        elif any(t in msg_lower for t in ["casa", "resid√™ncia"]):
            property_type = "casa"
        
        # Transa√ß√£o
        transaction_type = None
        if any(t in msg_lower for t in ["alugar", "aluguel", "loca√ß√£o"]):
            transaction_type = "locacao"
        elif any(t in msg_lower for t in ["comprar", "venda", "financiamento"]):
            transaction_type = "venda"
        
        return PropertyHypothesis(
            neighborhood=neighborhood,
            bedrooms=bedrooms,
            property_type=property_type,
            transaction_type=transaction_type,
            urgency_score=urgency_score,
            intent_confidence=0.7,
            extracted_keywords=re.findall(r'\\b\\w{4,}\\b', msg_lower)[:5]
        )
    
    async def _directed_vector_search(self, 
                                    hypothesis: PropertyHypothesis, 
                                    original_query: str) -> List[Dict]:
        """Query vetorial dirigida com filtros da hip√≥tese"""
        
        try:
            # Construir query otimizada baseada na hip√≥tese
            search_parts = []
            
            if hypothesis.property_type:
                search_parts.append(hypothesis.property_type)
            
            if hypothesis.neighborhood:
                search_parts.append(hypothesis.neighborhood)
            
            if hypothesis.bedrooms:
                search_parts.append(f"{hypothesis.bedrooms} quartos")
            
            if hypothesis.transaction_type:
                search_parts.append(hypothesis.transaction_type)
            
            # Query h√≠brida: hip√≥tese + query original
            if search_parts:
                directed_query = " ".join(search_parts) + " " + original_query[:100]
            else:
                directed_query = original_query
            
            # Filtros din√¢micos
            filters = {"status": "active"}  # Sempre apenas ativos
            
            # Filtro de pre√ßo
            if hypothesis.min_price or hypothesis.max_price:
                filters["price_range"] = {
                    "min": hypothesis.min_price,
                    "max": hypothesis.max_price
                }
            
            # Filtro temporal: s√≥ im√≥veis atualizados nas √∫ltimas 6h
            six_hours_ago = datetime.utcnow() - timedelta(hours=6)
            filters["updated_at"] = {"$gte": six_hours_ago}
            
            # RAG dirigido com filtros
            results = await rag.retrieve(
                query=directed_query,
                top_k=5,  # Top-3 + 2 compar√°veis
                filters=filters
            )
            
            logger.info(f"RAG dirigido: {len(results)} resultados para '{directed_query}'")
            return results
            
        except Exception as e:
            logger.error(f"Erro na busca dirigida: {e}")
            
            # Fallback: busca simples
            return await rag.retrieve(original_query, top_k=5, filters={"status": "active"})
    
    async def _generate_top_seller_response(self,
                                          original_query: str,
                                          hypothesis: PropertyHypothesis, 
                                          properties: List[Dict],
                                          history: List[Dict] = None) -> str:
        """Gera resposta com tom de corretor top-vendedor"""
        
        # Prompt especializado para vendas
        system_prompt = """
        Voc√™ √© Sofia, corretor top-vendedor da Allega Im√≥veis (200+ vendas/ano).
        
        PERSONALIDADE:
        - Confiante mas n√£o arrogante
        - Cria urg√™ncia sem pressionar
        - Sempre oferece 2-3 op√ß√µes espec√≠ficas  
        - Agenda visita no final (call-to-action forte)
        
        REGRAS:
        - Se urg√™ncia alta (4-5): mencione "entendo a urg√™ncia" + disponibilidade imediata
        - Se cliente espec√≠fico: foque nas caracter√≠sticas exatas
        - Se gen√©rico: eduque sobre mercado + ofere√ßa op√ß√µes
        - SEMPRE termine com agendamento concreto
        
        FORMATO DA RESPOSTA:
        1. Reconhe√ßa necessidade espec√≠fica (1 linha)
        2. Apresente 2-3 im√≥veis com destaque (2-3 linhas cada)
        3. Call-to-action forte para visita (1-2 linhas)
        """
        
        # Construir contexto dos im√≥veis
        properties_context = ""
        for i, prop in enumerate(properties[:3], 1):
            meta = prop.get("meta", prop.get("metadata", {}))
            properties_context += f"""
            IM√ìVEL {i}:
            Descri√ß√£o: {prop.get('text', '')[:200]}
            Bairro: {meta.get('neighborhood', 'N/A')}
            Pre√ßo: {meta.get('price', 'Consulte')}
            URL: {meta.get('url', '')}
            Imagem: {meta.get('main_image', '')}
            """
        
        # Contexto da urg√™ncia
        urgency_context = ""
        if hypothesis.urgency_score >= 4:
            urgency_context = "\\nüö® CLIENTE COM URG√äNCIA ALTA - Oferecer visita HOJE/AMANH√É"
        
        full_prompt = f"""
        {system_prompt}
        
        PERGUNTA CLIENTE: "{original_query}"
        HIP√ìTESE EXTRA√çDA: Bairro={hypothesis.neighborhood}, Tipo={hypothesis.property_type}, Urg√™ncia={hypothesis.urgency_score}
        {urgency_context}
        
        IM√ìVEIS DISPON√çVEIS:
        {properties_context}
        
        RESPOSTA SOFIA (Tom top-vendedor):
        """
        
        try:
            model = os.getenv("OPENAI_FINETUNED_MODEL", "ft:gpt-4.1-mini-2025-04-14:personal:alloha-sofia-v1:CMFHyUpi")
            response = await asyncio.to_thread(rag.call_gpt, full_prompt, model)
            
            # Adicionar tags especiais para urg√™ncia
            if hypothesis.urgency_score >= 4:
                response = "<HOT> " + response + " <URGENT>"
            
            return response.strip()
            
        except Exception as e:
            logger.error(f"Erro na resposta top-seller: {e}")
            return self._generate_fallback_response(properties)
    
    def _get_phone_hash(self, phone: str) -> str:
        """Hash do telefone para privacy"""
        return hashlib.sha256(phone.encode()).hexdigest()[:12]
    
    def _get_session_cache(self, phone_hash: str, hypothesis: PropertyHypothesis) -> Optional[List[Dict]]:
        """Recuperar cache de sess√£o se compat√≠vel"""
        
        cache_entry = self.session_cache.get(phone_hash)
        if not cache_entry:
            return None
        
        # Verificar TTL
        if datetime.utcnow() - cache_entry["timestamp"] > timedelta(hours=self.cache_ttl_hours):
            del self.session_cache[phone_hash]
            return None
        
        # Verificar compatibilidade da hip√≥tese
        cached_hypothesis = cache_entry["hypothesis"]
        
        # Compatible se mesmo bairro + tipo + faixa de quartos
        if (cached_hypothesis.neighborhood == hypothesis.neighborhood and
            cached_hypothesis.property_type == hypothesis.property_type and
            abs((cached_hypothesis.bedrooms or 0) - (hypothesis.bedrooms or 0)) <= 1):
            
            return cache_entry["properties"]
        
        return None
    
    def _update_session_cache(self, phone_hash: str, properties: List[Dict], hypothesis: PropertyHypothesis):
        """Atualizar cache de sess√£o"""
        
        self.session_cache[phone_hash] = {
            "properties": properties[:50],  # √öltimos 50 im√≥veis
            "hypothesis": hypothesis,
            "timestamp": datetime.utcnow()
        }
        
        # Limpar cache antigo
        cutoff = datetime.utcnow() - timedelta(hours=self.cache_ttl_hours * 2)
        expired_keys = [
            k for k, v in self.session_cache.items()
            if v["timestamp"] < cutoff
        ]
        for key in expired_keys:
            del self.session_cache[key]
    
    def _generate_fallback_response(self, properties: List[Dict]) -> str:
        """Resposta de fallback simples"""
        
        if not properties:
            return (
                "Vou procurar as melhores op√ß√µes para voc√™! "
                "Me conte mais sobre suas prefer√™ncias de bairro e or√ßamento "
                "para encontrar o im√≥vel perfeito. "
                "Posso agendar uma consulta personalizada ainda hoje?"
            )
        
        return (
            f"Encontrei {len(properties)} √≥timas op√ß√µes para voc√™! "
            "Vou te enviar os detalhes dos melhores im√≥veis. "
            "Que tal agendarmos uma visita ainda esta semana? "
            "Tenho hor√°rios dispon√≠veis amanh√£ de manh√£ ou √† tarde."
        )
    
    async def _fallback_response(self, message: str, phone: str) -> Dict[str, Any]:
        """Sistema de fallback completo"""
        
        properties = await rag.retrieve(message, top_k=3, filters={"status": "active"})
        
        return {
            "response": self._generate_fallback_response(properties),
            "properties": properties,
            "comparable_properties": [],
            "hypothesis": self._generate_fallback_hypothesis(message),
            "urgency_detected": False,
            "latency_ms": 0,
            "cache_hit": False
        }
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """Estat√≠sticas do cache de sess√£o"""
        
        active_sessions = len(self.session_cache)
        total_properties_cached = sum(
            len(entry["properties"]) 
            for entry in self.session_cache.values()
        )
        
        return {
            "active_sessions": active_sessions,
            "total_properties_cached": total_properties_cached,
            "cache_ttl_hours": self.cache_ttl_hours,
            "hypothesis_cache_size": len(self.hypothesis_cache)
        }

# Inst√¢ncia global
dual_stack_intelligence = DualStackIntelligence()